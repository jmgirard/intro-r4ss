---
format: 
  revealjs:
    css: ../../styles.css
    slide-number: true
    show-slide-number: all
    progress: true
    history: true
    hash-type: number
    theme: default
    code-block-background: true
    highlight-style: github
    code-link: false
    code-copy: true
    controls: true
    pagetitle: "Intro R4SS Day 2B"
    author-meta: "Jeffrey Girard"
    date-meta: "2023-06-02"
---

::: {.my-title}
# [Introduction to R]{.blue2} <br />for Social Scientists

::: {.my-grey}
[Workshop Day 2B | 2023-06-02]{}<br />
[Jeffrey M. Girard | Pitt Methods]{}
:::

![](../../img/proud_coder_2780E3.svg){.absolute bottom=0 right=0 width=400}
:::

## Basic Regression {.smaller}

::: {.columns .pv4}
::: {.column width="60%"}
-   [Basic regression]{.b .blue} predicts one variable $y$ from another variable $x$ using a straight line

::: {.fragment .mt1}
-   This line is defined by two parameters
    -   The [intercept]{.b .green} is the value of $y$ when $x=0$
    -   The [slope]{.b .green} is the change in $y$ expected for a change of 1 in $x$ (from $x=0$ to $x=1$)
:::

::: {.fragment .mt1}
-   We will use `lm()` to fit regression models
    -   This will solve using ordinary least squares
    -   We need to give it the [data]{.b .green} and a [formula]{.b .green}
:::
:::

::: {.column .tc .pv5 width="40%"}
{{< li ogfgksuz trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}
:::
:::



## Basic Regression Live Coding

```{r}
#| echo: true
#| eval: false
#| error: true
#| code-line-numbers: false

# SETUP: Load the used packages (if needed) and read in the example dataset

library(tidyverse)
library(easystats)

yearspubs <- read_csv("yearspubs.csv")
yearspubs

# ==============================================================================

# LESSON: Regression is a special case of the linear model, so we use lm()

fit <- lm(
  formula = salary ~ years_since_phd,
  data = yearspubs
)

# ==============================================================================

# TIP: Get a parameter summary using model_parameters()

fit

model_parameters(fit)

# ==============================================================================

# TIP: Get effect sizes (standardized results) using standardize = "refit"

model_parameters(fit, standardize = "refit")

# ==============================================================================

# TIP: Get a performance summary using model_performance()

model_performance(fit)

# ==============================================================================

# TIP: Visualize the model expectations using estimate_expectation()

plot(estimate_expectation(fit))

# ==============================================================================

# LESSON: To center the predictor, use center()

yearspubs$years_c = center(yearspubs$years_since_phd)
  
fit_c <- lm(
  formula = salary ~ years_c,
  data = yearspubs
)

model_parameters(fit_c)

plot(estimate_expectation(fit))
plot(estimate_expectation(fit_c))

# ==============================================================================

# NOTE: Centering will only change the intercept in basic regression

compare_parameters(fit, fit_c)

compare_performance(fit, fit_c)
```


## Multiple Regression {.smaller}

::: {.columns .pv4}
::: {.column width="60%"}
-   We can also include [multiple predictors]{.b .blue} to assess the [partial effect]{.b .green} of each predictor
    -   This allows us to account for the variance shared by the predictors and the outcome

::: {.fragment .mt1}
-   This changes the slopes' interpretations
    -   The slope of $x_1$ is no longer just the change in $y$ expected for a change of 1 in $x_1$
    -   It is now the change in $y$ expected for a change of 1 in $x_1$ **when controlling for $x_2$**
    -   The slope of $x_2$ similarly controls for $x_1$
:::
:::

::: {.column .tc .pv5 width="40%"}
{{< li jmkpuued trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}
:::
:::


## Multiple Regression {.smaller}

::: {.columns .pv4}
::: {.column width="45%"}
- In the model $y \sim x_1$
    -   The $x_1$ slope captures [b]{.b .green} + [c]{.b .green}

::: {.fragment .mt1}
- In the model $y \sim x_2$
    -   The $x_2$ slope captures [c]{.b .green} + [f]{.b .green}
:::

::: {.fragment .mt1}
- In the model $y \sim x_1 + x_2$
    -   The $x_1$ slope captures [b]{.b .green} only
    -   The $x_2$ slope captures [f]{.b .green} only
    -   So the overlap of [c]{.b .green} is removed
:::

:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
![](../../img/venn.png)
:::
:::


## Multiple Regression Live Coding

```{r}
#| echo: true
#| eval: false
#| error: true
#| code-line-numbers: false

# SETUP: Load the used packages (if needed) and read in the example dataset

library(tidyverse)
library(easystats)

# yearspubs <- read_csv("yearspubs.csv")
# yearspubs

# ==============================================================================

# LESSON: To add more predictors to the formula, just separate them by +

fit_yn <- lm(
  formula = salary ~ years_since_phd + n_pubs,
  data = yearspubs
)

model_parameters(fit_yn)

model_parameters(fit_yn, standardize = "refit")

model_performance(fit_yn)

# ==============================================================================

# USECASE: We can compare our models in terms of parameters and performance

fit_y <- lm(salary ~ years_since_phd, data = yearspubs)
fit_n <- lm(salary ~ n_pubs, data = yearspubs)

compare_parameters(fit_yn, fit_y, fit_n)

compare_performance(fit_yn, fit_y, fit_n)
```


## Categorical Predictors {.smaller}

::: {.columns .pv4}
::: {.column width="60%"}
-   To include categorical predictors in regression models, we can use [dummy coding]{.b .blue}
    -   This creates binary predictor variables

::: {.fragment .mt1}
-   One [reference group]{.b .green} does not get a slope
    -   Instead, it controls the model intercept
    -   All other groups' slopes are just deviations from the intercept
:::

::: {.fragment .mt1}
-   There is no need to create dummy codes in R
    -   Just include a [factor]{.b .green} as a predictor variable
    -   The **first level** will be the reference group
:::
:::

::: {.column .tc .pv5 width="40%"}
{{< li cdbgwqyw trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}
:::
:::


## Categorical Live Coding

```{r}
#| echo: true
#| eval: false
#| error: true
#| code-line-numbers: false

# SETUP: Load the used packages (if needed) and read in the example dataset

library(tidyverse)
library(easystats)

# install.packages("palmerpenguins")
library(palmerpenguins)

# ==============================================================================

# USECASE: Compare three groups with regression (instead of oneway anova)

penguins

levels(penguins$species)

fit <- lm(body_mass_g ~ species, data = penguins)

model_parameters(fit) # estimate intercept and slopes

estimate_means(fit, at = "species") # estimate means

model_parameters(fit, standardize = "refit") # estimate standardized effects

model_performance(fit) # estimate model performance

plot(estimate_expectation(fit)) # plot model expectations

anova <- aov(fit) # refit as ANOVA
model_parameters(anova) # recreate F-test

# ==============================================================================

# USECASE: Compare the three groups from regression

estimate_contrasts(fit, contrast = "species")

# ==============================================================================

# USECASE: Including multiple categorical predictors

fit2 <- lm(body_mass_g ~ species + sex, data = penguins)

model_parameters(fit2)

estimate_contrasts(fit2, contrast = "species")
estimate_contrasts(fit2, contrast = "sex")

plot(estimate_expectation(fit2))

# ==============================================================================

# USECASE: Regression can even mix categorical and continuous predictors!

fit3 <- lm(body_mass_g ~ flipper_length_mm + species, data = penguins)

model_parameters(fit3)

model_parameters(fit3, standardize = "refit") 

model_performance(fit3)

plot(estimate_expectation(fit3))
```


## Interaction Effects {.smaller}

::: {.columns .pv4}
::: {.column width="60%"}
-   We may want to know if the effect of one predictor [depends on]{.b .green} the value on another predictor
    -   Does the effect of hours of **exercise** on weight loss *depend on* biological **sex**?
    -   Does the effect of hours of **exercise** on weight loss *depend on* the **effort** put in?

::: {.fragment .mt1}
-   To answer these, we can test [interaction effects]{.b .blue}
    -   Interaction effects are just slopes for the [product]{.b .green} of two or more predictors
    -   We can include continuous and categorical predictors in interaction effects
:::
:::

::: {.column .tc .pv5 width="40%"}
{{< li nmlpnruz trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}
:::
:::


## Interactions Live Coding

```{r}
#| echo: true
#| eval: false
#| error: true
#| code-line-numbers: false

# SETUP: Load the used packages (if needed) and read in the example dataset

library(tidyverse)
library(easystats)

exercise <- read_csv("exercise.csv")
exercise

# ==============================================================================

# LESSON: Fit a model with no interaction effect for comparison

fit1 <- lm(loss ~ hours + sex, data = exercise)

model_parameters(fit1)

plot(estimate_expectation(fit1))

estimate_slopes(fit1, trend = "hours", at = "sex")

# ==============================================================================

# LESSON: Does the effect of hours depend on sex (and vice versa)?

fit2 <- lm(loss ~ hours * sex, data = exercise)

model_parameters(fit2)

plot(estimate_expectation(fit2))

estimate_slopes(fit2, trend = "hours", at = "sex")

# ==============================================================================

# LESSON: Fit a model with no interaction effect for comparison

fit3 <- lm(loss ~ hours + effort, data = exercise)

model_parameters(fit3)

plot(estimate_expectation(fit3))

# NOTE: A model with no interaction will have parallel lines

# ==============================================================================

# LESSON: Does the effect of hours depend on effort (and vice versa)?

fit4 <- lm(loss ~ hours * effort, data = exercise)

model_parameters(fit4)

ggeffect(fit4, terms = c("hours", "effort")) |> plot() # put hours on x-axis

ggeffect(fit4, terms = c("effort", "hours")) |> plot() # put effort on x-axis

# ==============================================================================

# LESSON: Be cautious about higher-level interactions

fit5 <- lm(loss ~ hours * effort * sex, data = exercise)

model_parameters(fit5)

ggeffect(fit5, terms = c("hours", "effort", "sex")) |> plot()
```


## A Formula Resource {.smaller}

::: {.pv4}

<table width="100%">
<tr>
  <th>Formula</th>
  <th colspan=7>Slopes Estimated</th>
</tr>
<tr>
  <td width="30%">`y ~ x`</td>
  <td width="10%">$x$</td>
  <td width="10%"></td>
  <td width="10%"></td>
  <td width="10%"></td>
  <td width="10%"></td>
  <td width="10%"></td>
  <td width="10%"></td>
</tr>
<tr>
  <td>`y ~ x + w`</td>
  <td>$x$</td>
  <td>$w$</td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
</tr>
<tr>
  <td>`y ~ x * w`</td>
  <td>$x$</td>
  <td>$w$</td>
  <td></td>
  <td>$xw$</td>
  <td></td>
  <td></td>
  <td></td>
</tr>
<tr>
  <td>`y ~ x + w + z`</td>
  <td>$x$</td>
  <td>$w$</td>
  <td>$z$</td>
  <td></td>
  <td></td>
  <td></td>
  <td></td>
</tr>
<tr>
  <td>`y ~ x * w + z`</td>
  <td>$x$</td>
  <td>$w$</td>
  <td>$z$</td>
  <td>$xw$</td>
  <td></td>
  <td></td>
  <td></td>
</tr>
<tr>
  <td>`y ~ x * (w + z)`</td>
  <td>$x$</td>
  <td>$w$</td>
  <td>$z$</td>
  <td>$xw$</td>
  <td>$xz$</td>
  <td></td>
  <td></td>
</tr>
<tr>
  <td>`y ~ x * w * z`</td>
  <td>$x$</td>
  <td>$w$</td>
  <td>$z$</td>
  <td>$xw$</td>
  <td>$xz$</td>
  <td>$wz$</td>
  <td>$xwz$</td>
</tr>
</table>

::: {.pv4}
Note that predictors can be continuous (i.e., numbers) or categorical (i.e., factors), but if a categorical predictor has more than two levels, you will end up with additional slopes.
:::
:::


## Regression Diagnostics {.smaller}

::: {.columns .pv4}
::: {.column width="60%"}
-   LM makes some [assumptions]{.b .green} about the data
    -   If violated, this can lead to...
    -   Biased [coefficient]{.b} estimates
    -   Biased [standard error]{.b} estimates

::: {.fragment .mt1}
-   [Regression Diagnostics]{.b .blue} check for...
    -   Violated assumptions
    -   Outlier observations
    -   Multicollinearity
    -   Prediction quality
:::

:::

::: {.column .tc .pv5 width="40%"}
{{< li oamdefle trigger=loop delay=3000 colors=primary:#2a76dd class=rc >}}
:::
:::


## Diagnostics Live Coding

```{r}
#| echo: true
#| eval: false
#| error: true
#| code-line-numbers: false


fit <- lm(salary ~ yrs.since.phd + yrs.service, data = salaries)

# ==============================================================================

# USECASE: Check if the residuals appear normally distributed

fit |> check_normality()

fit |> check_normality() |> plot()

# ==============================================================================

# USECASE: Check if the residuals show heteroscedasticity (constant variance)

fit |> check_heteroscedasticity()

fit |> check_heteroscedasticity() |> plot()

# ==============================================================================

# USECASE: Check if the model's predictions match the observed distribution

fit |> check_predictions()

# ==============================================================================

# USECASE: Check if any of the predictors show collinearity with others

fit |> check_collinearity()

fit |> check_collinearity() |> plot()

# ==============================================================================

# USECASE: Check if any of the observations would be considered outliers

fit |> check_outliers()

fit |> check_outliers() |> plot()
```
