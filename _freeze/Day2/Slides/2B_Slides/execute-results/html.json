{
  "hash": "f8df7a87dedfca259318b9ed07fd5f2c",
  "result": {
    "markdown": "---\nformat: \n  revealjs:\n    css: ../../styles.css\n    slide-number: true\n    show-slide-number: all\n    progress: true\n    history: true\n    hash-type: number\n    theme: default\n    code-block-background: true\n    highlight-style: github\n    code-link: false\n    code-copy: true\n    controls: true\n    pagetitle: \"Intro R4SS Day 2B\"\n    author-meta: \"Jeffrey Girard\"\n    date-meta: \"2023-06-02\"\n---\n\n\n::: {.my-title}\n# [Introduction to R]{.blue2} <br />for Social Scientists\n\n::: {.my-grey}\n[Workshop Day 2B | 2023-06-02]{}<br />\n[Jeffrey M. Girard | Pitt Methods]{}\n:::\n\n![](../../img/proud_coder_2780E3.svg){.absolute bottom=0 right=0 width=400}\n:::\n\n## Basic Regression {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   [Basic regression]{.b .blue} predicts one variable $y$ from another variable $x$ using a straight line\n\n::: {.fragment .mt1}\n-   This line is defined by two parameters\n    -   The [intercept]{.b .green} is the value of $y$ when $x=0$\n    -   The [slope]{.b .green} is the change in $y$ expected for a change of 1 in $x$ (from $x=0$ to $x=1$)\n:::\n\n::: {.fragment .mt1}\n-   We will use `lm()` to fit regression models\n    -   This will solve using ordinary least squares\n    -   We need to give it the [data]{.b .green} and a [formula]{.b .green}\n:::\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li ogfgksuz trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n\n\n## Basic Regression Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# SETUP: Load the used packages (if needed) and read in the example dataset\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nyearspubs <- read_csv(\"yearspubs.csv\")\nyearspubs\n\n# ==============================================================================\n\n# LESSON: Regression is a special case of the linear model, so we use lm()\n\nfit <- lm(\n  formula = salary ~ years_since_phd,\n  data = yearspubs\n)\n\n# ==============================================================================\n\n# TIP: Get a parameter summary using model_parameters()\n\nfit\n\nmodel_parameters(fit)\n\n# ==============================================================================\n\n# TIP: Get effect sizes (standardized results) using standardize = \"refit\"\n\nmodel_parameters(fit, standardize = \"refit\")\n\n# ==============================================================================\n\n# TIP: Get a performance summary using model_performance()\n\nmodel_performance(fit)\n\n# ==============================================================================\n\n# TIP: Visualize the model expectations using estimate_expectation()\n\nplot(estimate_expectation(fit))\n\n# ==============================================================================\n\n# LESSON: To center the predictor, use center()\n\nyearspubs$years_c = center(yearspubs$years_since_phd)\n  \nfit_c <- lm(\n  formula = salary ~ years_c,\n  data = yearspubs\n)\n\nmodel_parameters(fit_c)\n\nplot(estimate_expectation(fit))\nplot(estimate_expectation(fit_c))\n\n# ==============================================================================\n\n# NOTE: Centering will only change the intercept in basic regression\n\ncompare_parameters(fit, fit_c)\n\ncompare_performance(fit, fit_c)\n```\n:::\n\n\n\n## Multiple Regression {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   We can also include [multiple predictors]{.b .blue} to assess the [partial effect]{.b .green} of each predictor\n    -   This allows us to account for the variance shared by the predictors and the outcome\n\n::: {.fragment .mt1}\n-   This changes the slopes' interpretations\n    -   The slope of $x_1$ is no longer just the change in $y$ expected for a change of 1 in $x_1$\n    -   It is now the change in $y$ expected for a change of 1 in $x_1$ **when controlling for $x_2$**\n    -   The slope of $x_2$ similarly controls for $x_1$\n:::\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li jmkpuued trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n\n## Multiple Regression {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"45%\"}\n- In the model $y \\sim x_1$\n    -   The $x_1$ slope captures [b]{.b .green} + [c]{.b .green}\n\n::: {.fragment .mt1}\n- In the model $y \\sim x_2$\n    -   The $x_2$ slope captures [c]{.b .green} + [f]{.b .green}\n:::\n\n::: {.fragment .mt1}\n- In the model $y \\sim x_1 + x_2$\n    -   The $x_1$ slope captures [b]{.b .green} only\n    -   The $x_2$ slope captures [f]{.b .green} only\n    -   So the overlap of [c]{.b .green} is removed\n:::\n\n:::\n\n::: {.column width=\"10%\"}\n:::\n\n::: {.column width=\"45%\"}\n![](../../img/venn.png)\n:::\n:::\n\n\n## Multiple Regression Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# SETUP: Load the used packages (if needed) and read in the example dataset\n\nlibrary(tidyverse)\nlibrary(easystats)\n\n# yearspubs <- read_csv(\"yearspubs.csv\")\n# yearspubs\n\n# ==============================================================================\n\n# LESSON: To add more predictors to the formula, just separate them by +\n\nfit_yn <- lm(\n  formula = salary ~ years_since_phd + n_pubs,\n  data = yearspubs\n)\n\nmodel_parameters(fit_yn)\n\nmodel_parameters(fit_yn, standardize = \"refit\")\n\nmodel_performance(fit_yn)\n\n# ==============================================================================\n\n# USECASE: We can compare our models in terms of parameters and performance\n\nfit_y <- lm(salary ~ years_since_phd, data = yearspubs)\nfit_n <- lm(salary ~ n_pubs, data = yearspubs)\n\ncompare_parameters(fit_yn, fit_y, fit_n)\n\ncompare_performance(fit_yn, fit_y, fit_n)\n```\n:::\n\n\n\n## Categorical Predictors {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   To include categorical predictors in regression models, we can use [dummy coding]{.b .blue}\n    -   This creates binary predictor variables\n\n::: {.fragment .mt1}\n-   One [reference group]{.b .green} does not get a slope\n    -   Instead, it controls the model intercept\n    -   All other groups' slopes are just deviations from the intercept\n:::\n\n::: {.fragment .mt1}\n-   There is no need to create dummy codes in R\n    -   Just include a [factor]{.b .green} as a predictor variable\n    -   The **first level** will be the reference group\n:::\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li cdbgwqyw trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n\n## Categorical Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# SETUP: Load the used packages (if needed) and read in the example dataset\n\nlibrary(tidyverse)\nlibrary(easystats)\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\n\n# ==============================================================================\n\n# USECASE: Compare three groups with regression (instead of oneway anova)\n\npenguins\n\nlevels(penguins$species)\n\nfit <- lm(body_mass_g ~ species, data = penguins)\n\nmodel_parameters(fit) # estimate intercept and slopes\n\nestimate_means(fit, at = \"species\") # estimate means\n\nmodel_parameters(fit, standardize = \"refit\") # estimate standardized effects\n\nmodel_performance(fit) # estimate model performance\n\nplot(estimate_expectation(fit)) # plot model expectations\n\nanova <- aov(fit) # refit as ANOVA\nmodel_parameters(anova) # recreate F-test\n\n# ==============================================================================\n\n# USECASE: Compare the three groups from regression\n\nestimate_contrasts(fit, contrast = \"species\")\n\n# ==============================================================================\n\n# USECASE: Including multiple categorical predictors\n\nfit2 <- lm(body_mass_g ~ species + sex, data = penguins)\n\nmodel_parameters(fit2)\n\nestimate_contrasts(fit2, contrast = \"species\")\nestimate_contrasts(fit2, contrast = \"sex\")\n\nplot(estimate_expectation(fit2))\n\n# ==============================================================================\n\n# USECASE: Regression can even mix categorical and continuous predictors!\n\nfit3 <- lm(body_mass_g ~ flipper_length_mm + species, data = penguins)\n\nmodel_parameters(fit3)\n\nmodel_parameters(fit3, standardize = \"refit\") \n\nmodel_performance(fit3)\n\nplot(estimate_expectation(fit3))\n```\n:::\n\n\n\n## Interaction Effects {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   We may want to know if the effect of one predictor [depends on]{.b .green} the value on another predictor\n    -   Does the effect of hours of **exercise** on weight loss *depend on* biological **sex**?\n    -   Does the effect of hours of **exercise** on weight loss *depend on* the **effort** put in?\n\n::: {.fragment .mt1}\n-   To answer these, we can test [interaction effects]{.b .blue}\n    -   Interaction effects are just slopes for the [product]{.b .green} of two or more predictors\n    -   We can include continuous and categorical predictors in interaction effects\n:::\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li nmlpnruz trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n\n## Interactions Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# SETUP: Load the used packages (if needed) and read in the example dataset\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nexercise <- read_csv(\"exercise.csv\")\nexercise\n\n# ==============================================================================\n\n# LESSON: Fit a model with no interaction effect for comparison\n\nfit1 <- lm(loss ~ hours + sex, data = exercise)\n\nmodel_parameters(fit1)\n\nplot(estimate_expectation(fit1))\n\nestimate_slopes(fit1, trend = \"hours\", at = \"sex\")\n\n# ==============================================================================\n\n# LESSON: Does the effect of hours depend on sex (and vice versa)?\n\nfit2 <- lm(loss ~ hours * sex, data = exercise)\n\nmodel_parameters(fit2)\n\nplot(estimate_expectation(fit2))\n\nestimate_slopes(fit2, trend = \"hours\", at = \"sex\")\n\n# ==============================================================================\n\n# LESSON: Fit a model with no interaction effect for comparison\n\nfit3 <- lm(loss ~ hours + effort, data = exercise)\n\nmodel_parameters(fit3)\n\nplot(estimate_expectation(fit3))\n\n# NOTE: A model with no interaction will have parallel lines\n\n# ==============================================================================\n\n# LESSON: Does the effect of hours depend on effort (and vice versa)?\n\nfit4 <- lm(loss ~ hours * effort, data = exercise)\n\nmodel_parameters(fit4)\n\nggeffect(fit4, terms = c(\"hours\", \"effort\")) |> plot() # put hours on x-axis\n\nggeffect(fit4, terms = c(\"effort\", \"hours\")) |> plot() # put effort on x-axis\n\n# ==============================================================================\n\n# LESSON: Be cautious about higher-level interactions\n\nfit5 <- lm(loss ~ hours * effort * sex, data = exercise)\n\nmodel_parameters(fit5)\n\nggeffect(fit5, terms = c(\"hours\", \"effort\", \"sex\")) |> plot()\n```\n:::\n\n\n\n## A Formula Resource {.smaller}\n\n::: {.pv4}\n\n<table width=\"100%\">\n<tr>\n  <th>Formula</th>\n  <th colspan=7>Slopes Estimated</th>\n</tr>\n<tr>\n  <td width=\"30%\">`y ~ x`</td>\n  <td width=\"10%\">$x$</td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n</tr>\n<tr>\n  <td>`y ~ x + w`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td></td>\n  <td></td>\n  <td></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x * w`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td></td>\n  <td>$xw$</td>\n  <td></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x + w + z`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td>$z$</td>\n  <td></td>\n  <td></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x * w + z`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td>$z$</td>\n  <td>$xw$</td>\n  <td></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x * (w + z)`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td>$z$</td>\n  <td>$xw$</td>\n  <td>$xz$</td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x * w * z`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td>$z$</td>\n  <td>$xw$</td>\n  <td>$xz$</td>\n  <td>$wz$</td>\n  <td>$xwz$</td>\n</tr>\n</table>\n\n::: {.pv4}\nNote that predictors can be continuous (i.e., numbers) or categorical (i.e., factors), but if a categorical predictor has more than two levels, you will end up with additional slopes.\n:::\n:::\n\n\n## Regression Diagnostics {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   LM makes some [assumptions]{.b .green} about the data\n    -   If violated, this can lead to...\n    -   Biased [coefficient]{.b} estimates\n    -   Biased [standard error]{.b} estimates\n\n::: {.fragment .mt1}\n-   [Regression Diagnostics]{.b .blue} check for...\n    -   Violated assumptions\n    -   Outlier observations\n    -   Multicollinearity\n    -   Prediction quality\n:::\n\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li oamdefle trigger=loop delay=3000 colors=primary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n\n## Diagnostics Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nfit <- lm(salary ~ yrs.since.phd + yrs.service, data = salaries)\n\n# ==============================================================================\n\n# USECASE: Check if the residuals appear normally distributed\n\nfit |> check_normality()\n\nfit |> check_normality() |> plot()\n\n# ==============================================================================\n\n# USECASE: Check if the residuals show heteroscedasticity (constant variance)\n\nfit |> check_heteroscedasticity()\n\nfit |> check_heteroscedasticity() |> plot()\n\n# ==============================================================================\n\n# USECASE: Check if the model's predictions match the observed distribution\n\nfit |> check_predictions()\n\n# ==============================================================================\n\n# USECASE: Check if any of the predictors show collinearity with others\n\nfit |> check_collinearity()\n\nfit |> check_collinearity() |> plot()\n\n# ==============================================================================\n\n# USECASE: Check if any of the observations would be considered outliers\n\nfit |> check_outliers()\n\nfit |> check_outliers() |> plot()\n```\n:::\n",
    "supporting": [
      "2B_Slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}