{
  "hash": "19b5738bbdde746263f571be3a8368d8",
  "result": {
    "markdown": "---\nformat: \n  revealjs:\n    css: ../../styles.css\n    slide-number: true\n    show-slide-number: all\n    progress: true\n    history: true\n    hash-type: number\n    theme: default\n    code-block-background: true\n    highlight-style: github\n    code-link: false\n    code-copy: true\n    controls: true\n    pagetitle: \"Intro R4SS Day 2B\"\n    author-meta: \"Jeffrey Girard\"\n    date-meta: \"2023-06-02\"\n---\n\n\n::: {.my-title}\n# [Introduction to R]{.blue2} <br />for Social Scientists\n\n::: {.my-grey}\n[Workshop Day 2B | 2023-06-02]{}<br />\n[Jeffrey M. Girard | Pitt Methods]{}\n:::\n\n![](../../img/proud_coder_2780E3.svg){.absolute bottom=0 right=0 width=400}\n:::\n\n## Basic Regression {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   [Basic regression]{.b .blue} predicts one variable $y$ from another variable $x$ using a straight line\n\n::: {.fragment .mt1}\n-   This line is defined by two parameters\n    -   The [intercept]{.b .green} is the value of $y$ when $x=0$\n    -   The [slope]{.b .green} is the change in $y$ expected for a change of 1 in $x$ (from $x=0$ to $x=1$)\n:::\n\n::: {.fragment .mt1}\n-   We will use `lm()` to fit regression models\n    -   This will solve using ordinary least squares\n    -   We need to give it the [data]{.b .green} and a [formula]{.b .green}\n:::\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li ogfgksuz trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n\n## Basic Regression Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# SETUP: Load the used packages (if needed) and read in the example dataset\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nyearspubs <- read_csv(\"yearspubs.csv\")\nyearspubs\n\n# ==============================================================================\n\n# LESSON: Regression is a special case of the linear model, so we use lm()\n\nfit <- lm(\n  formula = salary ~ years_since_phd,\n  data = yearspubs\n)\n\n# ==============================================================================\n\n# TIP: Get a parameter summary using model_parameters()\n\nfit\n\nmodel_parameters(fit)\n\n# ==============================================================================\n\n# TIP: Get effect sizes (standardized results) using standardize = \"refit\"\n\nmodel_parameters(fit, standardize = \"refit\")\n\n# ==============================================================================\n\n# TIP: Get a performance summary using model_performance()\n\nmodel_performance(fit)\n\n# ==============================================================================\n\n# TIP: Visualize the model expectations using estimate_relation()\n\nplot(estimate_relation(fit))\n\n# ==============================================================================\n\n# LESSON: To center the predictor, use center()\n\nyearspubs$years_since_phd = center(yearspubs$years_since_phd)\n  \nfit_c <- lm(\n  formula = salary ~ years_since_phd,\n  data = yearspubs\n)\n\nmodel_parameters(fit_c)\n\nplot(estimate_relation(fit))\nplot(estimate_relation(fit_c))\n\n# ==============================================================================\n\n# NOTE: Centering will only change the intercept in basic regression\n\ncompare_parameters(fit, fit_c)\n\ncompare_performance(fit, fit_c)\n```\n:::\n\n\n\n## Multiple Regression {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   We can also include [multiple predictors]{.b .blue} to assess the [partial effect]{.b .green} of each predictor\n    -   This allows us to account for the variance shared by the predictors and the outcome\n\n::: {.fragment .mt1}\n-   This changes the slopes' interpretations\n    -   The slope of $x_1$ is no longer just the change in $y$ expected for a change of 1 in $x_1$\n    -   It is now the change in $y$ expected for a change of 1 in $x_1$ **when controlling for $x_2$**\n    -   The slope of $x_2$ similarly controls for $x_1$\n:::\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li jmkpuued trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n\n## Multiple Regression {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"45%\"}\n- In the model $y \\sim x_1$\n    -   The $x_1$ slope captures [b]{.b .green} + [c]{.b .green}\n\n::: {.fragment .mt1}\n- In the model $y \\sim x_2$\n    -   The $x_2$ slope captures [c]{.b .green} + [f]{.b .green}\n:::\n\n::: {.fragment .mt1}\n- In the model $y \\sim x_1 + x_2$\n    -   The $x_1$ slope captures [b]{.b .green} only\n    -   The $x_2$ slope captures [f]{.b .green} only\n    -   So the overlap of [c]{.b .green} is removed\n:::\n\n:::\n\n::: {.column width=\"10%\"}\n:::\n\n::: {.column width=\"45%\"}\n![](../../img/venn.png)\n:::\n:::\n\n\n## Multiple Regression Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# SETUP: Load the used packages (if needed) and read in the example dataset\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nyearspubs\n\n# ==============================================================================\n\n# LESSON: To add more predictors to the formula, just separate them by +\n\nfit_yn <- lm(\n  formula = salary ~ years_since_phd + n_pubs,\n  data = yearspubs\n)\n\nmodel_parameters(fit_yn)\n\nmodel_parameters(fit_yn, standardize = \"refit\")\n\nmodel_performance(fit_yn)\n\n# ==============================================================================\n\n# USECASE: We can compare our models in terms of parameters and performance\n\nfit_y <- lm(salary ~ years_since_phd, data = yearspubs)\nfit_n <- lm(salary ~ n_pubs, data = yearspubs)\n\ncompare_parameters(fit_yn, fit_y, fit_n)\n\ncompare_performance(fit_yn, fit_y, fit_n)\n```\n:::\n\n\n\n## Categorical Predictors {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   To include categorical predictors in regression models, we can use [dummy coding]{.b .blue}\n    -   This creates binary predictor variables\n\n::: {.fragment .mt1}\n-   One [reference group]{.b .green} does not get a slope\n    -   Instead, it controls the model intercept\n    -   All other groups' slopes are just deviations from the intercept\n:::\n\n::: {.fragment .mt1}\n-   There is no need to create dummy codes in R\n    -   Just include a [factor]{.b .green} as a predictor variable\n    -   The **first level** will be the reference group\n:::\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li cdbgwqyw trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n\n## Categorical Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# SETUP: Load the used packages (if needed) and read in the example dataset\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(palmerpenguins)\n\n# ==============================================================================\n\n# USECASE: Compare three groups with regression (instead of oneway anova)\n\npenguins\n\nlevels(penguins$species)\n\nfit <- lm(body_mass_g ~ species, data = penguins)\n\nmodel_parameters(fit) # estimate intercept and slopes\n\nestimate_means(fit, at = \"species\") # estimate means\n\nmodel_parameters(fit, standardize = \"refit\") # estimate standardized effects\n\nmodel_performance(fit) # estimate model performance\n\nplot(estimate_relation(fit)) # plot model predictions\n\nanova <- aov(fit) # refit as ANOVA\nmodel_parameters(anova) # recreate F-test\n\n# ==============================================================================\n\n# USECASE: Compare the three groups from regression\n\nestimate_contrasts(fit, contrast = \"species\")\n\n# ==============================================================================\n\n# USECASE: Including multiple categorical predictors\n\nfit2 <- lm(body_mass_g ~ species + sex, data = penguins)\n\nmodel_parameters(fit2)\n\nestimate_contrasts(fit2, contrast = \"species\")\nestimate_contrasts(fit2, contrast = \"sex\")\n\nplot(estimate_relation(fit2))\n\n# ==============================================================================\n\n# USECASE: Regression can even mix categorical and continuous predictors!\n\nfit3 <- lm(body_mass_g ~ flipper_length_mm + species, data = penguins)\n\nmodel_parameters(fit3)\n\nmodel_parameters(fit3, standardize = \"refit\") \n\nmodel_performance(fit3)\n\nplot(estimate_relation(fit3))\n```\n:::\n\n\n\n## Interaction Effects {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   We may want to know if the effect of one predictor [depends on]{.b .green} the value on another predictor\n    -   Does the effect of hours of **exercise** on weight loss *depend on* biological **sex**?\n    -   Does the effect of hours of **exercise** on weight loss *depend on* the **effort** put in?\n\n::: {.fragment .mt1}\n-   To answer these, we can test [interaction effects]{.b .blue}\n    -   Interaction effects are just slopes for the [product]{.b .green} of two or more predictors\n    -   We can include continuous and categorical predictors in interaction effects\n:::\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li nmlpnruz trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n\n## Interactions Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# SETUP: Load the used packages (if needed) and read in the example dataset\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nexercise <- read_csv(\"exercise.csv\")\nexercise\n\n# ==============================================================================\n\n# LESSON: Fit a model with no interaction effect for comparison\n\nfit1 <- lm(loss ~ hours + sex, data = exercise)\n\nmodel_parameters(fit1)\n\nplot(estimate_relation(fit1))\n\n# ==============================================================================\n\n# LESSON: Does the effect of hours depend on sex (and vice versa)?\n\nfit2 <- lm(loss ~ hours * sex, data = exercise)\n\nmodel_parameters(fit2)\n\nplot(estimate_relation(fit2))\n\n# ==============================================================================\n\n# LESSON: Fit a model with no interaction effect for comparison\n\nfit3 <- lm(loss ~ hours + effort, data = exercise)\n\nmodel_parameters(fit3)\n\nplot(estimate_relation(fit3))\n\n# NOTE: A model with no interaction will have parallel lines\n\n# ==============================================================================\n\n# LESSON: Does the effect of hours depend on effort (and vice versa)?\n\nfit4 <- lm(loss ~ hours * effort, data = exercise)\n\nmodel_parameters(fit4)\n\nplot(estimate_relation(fit4))\n\nslopes <- estimate_slopes(fit4, trend = \"hours\", at = \"effort\")\nslopes\nplot(slopes)\n\n# TIP: Increase length to estimate more slopes and fill in gaps in the plot\n\nslopes <- estimate_slopes(fit4, trend = \"hours\", at = \"effort\", length = 300)\nplot(slopes)\n\n# TIP: Swap trend and at to see the reverse\n\nslopes <- estimate_slopes(fit4, trend = \"effort\", at = \"hours\", length = 300)\nplot(slopes)\n\n# ==============================================================================\n\n# LESSON: Be cautious about higher-level interactions\n\nfit5 <- lm(loss ~ hours * effort * sex, data = exercise)\n\nmodel_parameters(fit5)\n\nplot(estimate_relation(fit5))\n```\n:::\n\n\n\n## A Formula Resource {.smaller}\n\n::: {.pv4}\n\n<table width=\"100%\">\n<tr>\n  <th>Formula</th>\n  <th colspan=7>Slopes Estimated</th>\n</tr>\n<tr>\n  <td width=\"30%\">`y ~ x`</td>\n  <td width=\"10%\">$x$</td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n  <td width=\"10%\"></td>\n</tr>\n<tr>\n  <td>`y ~ x + w`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td></td>\n  <td></td>\n  <td></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x * w`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td></td>\n  <td>$xw$</td>\n  <td></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x + w + z`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td>$z$</td>\n  <td></td>\n  <td></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x * w + z`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td>$z$</td>\n  <td>$xw$</td>\n  <td></td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x * (w + z)`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td>$z$</td>\n  <td>$xw$</td>\n  <td>$xz$</td>\n  <td></td>\n  <td></td>\n</tr>\n<tr>\n  <td>`y ~ x * w * z`</td>\n  <td>$x$</td>\n  <td>$w$</td>\n  <td>$z$</td>\n  <td>$xw$</td>\n  <td>$xz$</td>\n  <td>$wz$</td>\n  <td>$xwz$</td>\n</tr>\n</table>\n\n::: {.pv4}\nNote that predictors can be continuous (i.e., numbers) or categorical (i.e., factors), but if a categorical predictor has more than two levels, you will end up with additional slopes.\n:::\n:::\n\n\n## Regression Diagnostics {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   LM makes some [assumptions]{.b .green} about the data\n    -   If violated, this can lead to...\n    -   Biased [coefficient]{.b} estimates\n    -   Biased [standard error]{.b} estimates\n\n::: {.fragment .mt1}\n-   [Regression Diagnostics]{.b .blue} check for...\n    -   Violated assumptions\n    -   Outlier observations\n    -   Multicollinearity\n    -   Prediction quality\n:::\n\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li oamdefle trigger=loop delay=3000 colors=primary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n\n## Diagnostics Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nyearspubs\n\nfit <- lm(salary ~ years_since_phd + n_pubs, data = yearspubs)\n\n# ==============================================================================\n\n# USECASE: Check model diagnostics\n\ncheck_model(fit)\n\n# TIP: Since this is a big figure, it may be helpful to click the \"Zoom\" button\n\n# ==============================================================================\n\n# EXPERIMENTAL: Still being developed, but very cool\n\n# install.packages(c(\"DT\", \"flexdashboard\"))\nmodel_dashboard(fit)\n```\n:::\n\n\n## Addressing Violations\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# ==============================================================================\n\n# LESSON: Bootstrapping to address non-normal residuals\n\nfit <- lm(n_pubs ~ years_since_phd, data = yearspubs)\n\nmodel_parameters(fit)\n\nplot(check_normality(fit))\n\n# install.packages(\"qqplotr\")\nplot(check_normality(fit), type = \"qq\")\n\nmodel_parameters(fit, bootstrap = TRUE)\n\n# ==============================================================================\n\n# LESSON: Robust standard errors for heteroskedasticity\n\n# Simulate some data is has increasing error variance\n\nset.seed(2023)\nsimdata <- tibble(\n  income = runif(n = 60, min = 0, max = 100),\n  rent = abs(5 + (3 * income + rnorm(n = 60, mean = 0, sd = income)) / 5)\n)\n\nqplot(x = income, y = rent, data = simdata, geom = \"point\")\n\n# Fit linear model\n\nfit2 <- lm(rent ~ 1 + income, data = simdata)\n\n# Estimate normal SEs (assuming constant error variance)\n\nmodel_parameters(fit2)\n\nplot(check_heteroskedasticity(fit2))\n\n# Estimate heteroskedasticity-robust SEs\n\n# install.packages(\"sandwich\")\nmodel_parameters(fit2, vcov = \"HC3\")\n\n# ==============================================================================\n\n# LESSON: Polynomial regression for non-linear functional form\n\n# Simulate some data that is quadratic (not linear)\n\nset.seed(2023)\nsimdata <- tibble(\n  cost = runif(n = 50, min = 100, max = 120),\n  x = cost - 112,\n  satisfaction = 10 + 2 * x - 1 * x^2 + rnorm(n = 50, m = 0, sd = 15)\n)\n\n# Fit a linear model anyway\n\nfit3a <- lm(satisfaction ~ cost, data = simdata)\n\nmodel_parameters(fit3a)\n\ncheck_model(fit3a)\n\nplot(estimate_relation(fit3a))\n\n# Now fit a polynomial model\n\nfit3b <- lm(satisfaction ~ poly(cost, degree = 2), data = simdata)\n\nmodel_parameters(fit3b)\n\ncheck_model(fit3b)\n\ner <- estimate_relation(fit3b, length = 300)\nplot(er)\n\n# Find where the inflection point is (in terms of the x-variable)\n\ndescribe_nonlinear(er, x = \"cost\")\n```\n:::\n\n\n## Simple Power Analysis {.smaller}\n\n::: {.columns .pv4}\n::: {.column width=\"60%\"}\n-   [Power]{.b .green} is the probability of [correctly rejecting]{.b .blue} the null hypothesis (given that that null is wrong)\n    -   Power is related to the effect size, $n$, and $\\alpha$\n    -   If we know three, we can estimate the fourth\n    -   *What sample size is needed for 80% power to detect an effect of the expected size or larger?*\n\n::: {.fragment .mt1}\n-   What [effect sizes]{.b .green} should we expect?\n    -   Ideally base the size on previous studies...\n    -   Cohen's $d$ for $t$-tests (e.g., 0.20, 0.50, 0.80)\n    -   Cohen's $f$ for ANOVAs (e.g., 0.10, 0.25, 0.40)\n    -   Cohen's $f^2$ for regression (e.g., 0.02, 0.15, 0.35)\n:::\n\n:::\n\n::: {.column .tc .pv5 width=\"40%\"}\n\n{{< li htqwuudr trigger=loop delay=3000 colors=secondary:#2a76dd class=rc >}}\n\n\n:::\n:::\n\n## Power Analysis Live Coding\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# install.packages(\"WebPower\")\nlibrary(WebPower)\n\n# ==============================================================================\n\n# t-tests\n\n## We want 80% power to detect a difference of d = 0.5 between 2 ind. groups\n\nwp.t(\n  power = 0.80,\n  alpha = 0.05,\n  type = \"two.sample\",\n  d = 0.5 # effect size\n)\n\n## We want 80% power to detect a difference of d = 0.5 between 2 dep. groups\n\nwp.t(\n  power = 0.80,\n  alpha = 0.05,\n  type = \"paired\",\n  d = 0.5 # effect size\n)\n\n# ==============================================================================\n\n# ANOVAs\n\n## We want 80% power to detect a difference of f = 0.25 between 4 ind. groups\n\nwp.anova(\n  power = 0.80,\n  alpha = 0.05,\n  type = \"overall\",\n  k = 4, # number of groups to compare\n  f = 0.25 # effect size\n)\n\n# ==============================================================================\n\n# Regression: Omnibus (overall F-test)\n\n## We want 80% power to explain 10% of the variance in y with 5 predictors\n\nr2_full <- 0.10\nr2_reduced <- 0\nf2 <- (r2_full - r2_reduced) / (1 - r2_full)\nf2\n\nwp.regression(\n  power = 0.80,\n  alpha = 0.05,\n  p1 = 5, # number of predictors in full model\n  p2 = 0, # number of predictors in reduced model\n  f2 = f2 # effect size\n)\n\n# ==============================================================================\n\n# Regression: Targeted (slope's t-test)\n\n## Imagine we can explain 10% of the variance in y using x1 and x2\n## We want 80% power to detect an effect of x3 that explains an additional 5%\n\nr2_full <- 0.15\nr2_reduced <- 0.10\nf2 <- (r2_full - r2_reduced) / (1 - r2_full)\nf2\n\nwp.regression(\n  power = 0.80,\n  alpha = 0.05,\n  p1 = 3, # number of predictors in full model\n  p2 = 2, # number of predictors in reduced model\n  f2 = f2 # effect size\n)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}